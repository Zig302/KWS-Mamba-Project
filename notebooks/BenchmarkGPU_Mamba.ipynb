{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U datasets\n",
        "!pip install datasets==2.16.0\n",
        "!pip install huggingface-hub==0.20.0\n",
        "!apt-get install -y libsox-dev\n",
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install causal-conv1d==1.4.0 && pip install mamba-ssm==2.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4INgCOm4vzf",
        "outputId": "5c086e6f-f9e7-4038-a37e-5212c7ef3b53",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.16.0\n",
            "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.16.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.0)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (0.70.16)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.0)\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (3.12.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (0.35.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.0) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.16.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.16.0) (2025.8.3)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.16.0)\n",
            "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
            "  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.16.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.0) (1.17.0)\n",
            "Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pyarrow-hotfix, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.16\n",
            "    Uninstalling multiprocess-0.70.16:\n",
            "      Successfully uninstalled multiprocess-0.70.16\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.16.0 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15 pyarrow-hotfix-0.7\n",
            "Collecting huggingface-hub==0.20.0\n",
            "  Downloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (2023.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.20.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.20.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.20.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.20.0) (2025.8.3)\n",
            "Downloading huggingface_hub-0.20.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.35.0\n",
            "    Uninstalling huggingface-hub-0.35.0:\n",
            "      Successfully uninstalled huggingface-hub-0.35.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.46.0 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "peft 0.17.1 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "accelerate 1.10.1 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "transformers 4.56.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "diffusers 0.35.1 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.20.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0\n",
            "  libsox-fmt-all libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base libsox-fmt-mp3\n",
            "  libsox-fmt-oss libsox-fmt-pulse libsox3 libwavpack1\n",
            "Suggested packages:\n",
            "  libaudio2 libsndio6.1\n",
            "The following NEW packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0\n",
            "  libsox-dev libsox-fmt-all libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base\n",
            "  libsox-fmt-mp3 libsox-fmt-oss libsox-fmt-pulse libsox3 libwavpack1\n",
            "0 upgraded, 16 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 1,053 kB of archives.\n",
            "After this operation, 4,061 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libao-common all 1.2.2+20180113-1.1ubuntu3 [6,568 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libao4 amd64 1.2.2+20180113-1.1ubuntu3 [35.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libid3tag0 amd64 0.15.1b-14 [31.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmad0 amd64 0.15.1b-10ubuntu1 [63.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [240 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [11.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-ao amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [7,740 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwavpack1 amd64 5.4.0-1build2 [83.7 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [33.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [17.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-oss amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [9,424 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-pulse amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [7,732 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-all amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [5,016 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-dev amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [356 kB]\n",
            "Fetched 1,053 kB in 2s (456 kB/s)\n",
            "Selecting previously unselected package libao-common.\n",
            "(Reading database ... 126441 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libao-common_1.2.2+20180113-1.1ubuntu3_all.deb ...\n",
            "Unpacking libao-common (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libao4:amd64.\n",
            "Preparing to unpack .../01-libao4_1.2.2+20180113-1.1ubuntu3_amd64.deb ...\n",
            "Unpacking libao4:amd64 (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../02-libid3tag0_0.15.1b-14_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../03-libmad0_0.15.1b-10ubuntu1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../04-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../05-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../06-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../07-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-ao:amd64.\n",
            "Preparing to unpack .../08-libsox-fmt-ao_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-ao:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../09-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../10-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../11-libsox-fmt-mp3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-oss:amd64.\n",
            "Preparing to unpack .../12-libsox-fmt-oss_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-oss:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-pulse:amd64.\n",
            "Preparing to unpack .../13-libsox-fmt-pulse_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-pulse:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-all:amd64.\n",
            "Preparing to unpack .../14-libsox-fmt-all_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-all:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-dev:amd64.\n",
            "Preparing to unpack .../15-libsox-dev_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-dev:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-oss:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libao-common (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libao4:amd64 (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-ao:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-pulse:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-all:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-dev:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp312-cp312-linux_x86_64.whl (799.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 MB\u001b[0m \u001b[31m812.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.19.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp312-cp312-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2023.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (75.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.8.0+cu126\n",
            "    Uninstalling torchaudio-2.8.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.17.1 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "accelerate 1.10.1 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0+cu121 torchaudio-2.4.0+cu121 torchvision-0.19.0+cu121 triton-3.0.0\n",
            "Collecting causal-conv1d==1.4.0\n",
            "  Downloading causal_conv1d-1.4.0.tar.gz (9.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from causal-conv1d==1.4.0) (2.4.0+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from causal-conv1d==1.4.0) (25.0)\n",
            "Collecting ninja (from causal-conv1d==1.4.0)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (2023.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.4.0) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->causal-conv1d==1.4.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->causal-conv1d==1.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->causal-conv1d==1.4.0) (1.3.0)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: causal-conv1d\n",
            "  Building wheel for causal-conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causal-conv1d: filename=causal_conv1d-1.4.0-cp312-cp312-linux_x86_64.whl size=104884589 sha256=70c6d91d00c625d750b83c9c4bdd0bd395c61aff9baa8c0d58e0a33dabbe3b4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/bd/04/a4893fd5ad69a02f55b49f04dc1dbbd7c439332db7895f62ff\n",
            "Successfully built causal-conv1d\n",
            "Installing collected packages: ninja, causal-conv1d\n",
            "Successfully installed causal-conv1d-1.4.0 ninja-1.13.0\n",
            "Collecting mamba-ssm==2.2.2\n",
            "  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.2.2) (2.4.0+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.2.2) (25.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.2.2) (1.13.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.2.2) (0.8.1)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.2.2) (3.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.2.2) (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (2023.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm==2.2.2) (12.6.85)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers->mamba-ssm==2.2.2)\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.2.2) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.2.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.2.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.2.2) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.2.2) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.2.2) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.2.2) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->mamba-ssm==2.2.2) (1.1.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->mamba-ssm==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm==2.2.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm==2.2.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm==2.2.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm==2.2.2) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->mamba-ssm==2.2.2) (1.3.0)\n",
            "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp312-cp312-linux_x86_64.whl size=324005633 sha256=f4cf06585ac28c783dbe6fe357c64218415af3665c3c06106be8f86dab69b2a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/99/4112d82fe2a9458dd194c3ff54c43a14a8594c8f90cf16046f\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: huggingface-hub, mamba-ssm\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.0\n",
            "    Uninstalling huggingface-hub-0.20.0:\n",
            "      Successfully uninstalled huggingface-hub-0.20.0\n",
            "Successfully installed huggingface-hub-0.35.3 mamba-ssm-2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVhXGw4X3lvg",
        "outputId": "cca56b5e-629a-4326-95db-c500d45b41d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment: Colab\n",
            "CUDA available: True\n",
            "Device: cuda\n",
            "✅ Using mamba_ssm library\n",
            "\n",
            "🚀 Starting Mamba KWS Inference Benchmark\n",
            "Device: cuda\n",
            "Using mamba_ssm: True\n",
            "============================================================\n",
            "\n",
            "📊 Benchmarking SMALL model...\n",
            "⚠️  Model file not found: content/small_kws_mel_97.42.pt\n",
            "Using random weights for small model (benchmark mode)\n",
            "Model parameters: 462,147\n",
            "🔍 Running latency benchmark (1000 runs)...\n",
            "📈 Running throughput benchmark...\n",
            "  Testing batch size 1...\n",
            "  Testing batch size 2...\n",
            "  Testing batch size 4...\n",
            "  Testing batch size 8...\n",
            "  Testing batch size 16...\n",
            "  Testing batch size 32...\n",
            "  Testing batch size 64...\n",
            "  Testing batch size 128...\n",
            "  Testing batch size 256...\n",
            "  Testing batch size 512...\n",
            "  Testing batch size 1028...\n",
            "  Testing batch size 2056...\n",
            "🧠 Running memory usage benchmark...\n",
            "\n",
            "📋 SMALL Results Summary:\n",
            "  Parameters: 462,147\n",
            "  Latency (single): 5.49ms ± 0.65ms\n",
            "  P95 latency: 6.59ms\n",
            "  Max throughput: 3289.6 samples/sec (batch=32)\n",
            "  Memory (batch=1): 2.1MB total, 2.1MB activations\n",
            "  Memory (batch=32): 105.5MB total, 3.30MB per sample\n",
            "\n",
            "📊 Benchmarking MEDIUM model...\n",
            "⚠️  Model file not found: content/medium_kws_melSpec_97.58.pt\n",
            "Using random weights for medium model (benchmark mode)\n",
            "Model parameters: 1,506,051\n",
            "🔍 Running latency benchmark (1000 runs)...\n",
            "📈 Running throughput benchmark...\n",
            "  Testing batch size 1...\n",
            "  Testing batch size 2...\n",
            "  Testing batch size 4...\n",
            "  Testing batch size 8...\n",
            "  Testing batch size 16...\n",
            "  Testing batch size 32...\n",
            "  Testing batch size 64...\n",
            "  Testing batch size 128...\n",
            "  Testing batch size 256...\n",
            "  Testing batch size 512...\n",
            "  Testing batch size 1028...\n",
            "  Testing batch size 2056...\n",
            "🧠 Running memory usage benchmark...\n",
            "\n",
            "📋 MEDIUM Results Summary:\n",
            "  Parameters: 1,506,051\n",
            "  Latency (single): 6.52ms ± 0.71ms\n",
            "  P95 latency: 7.59ms\n",
            "  Max throughput: 2796.5 samples/sec (batch=32)\n",
            "  Memory (batch=1): 2.1MB total, 2.1MB activations\n",
            "  Memory (batch=32): 105.5MB total, 3.30MB per sample\n",
            "\n",
            "📊 Benchmarking LARGE model...\n",
            "⚠️  Model file not found: content/large_kws_melSpec_97.75.pt\n",
            "Using random weights for large model (benchmark mode)\n",
            "Model parameters: 3,504,323\n",
            "🔍 Running latency benchmark (1000 runs)...\n",
            "📈 Running throughput benchmark...\n",
            "  Testing batch size 1...\n",
            "  Testing batch size 2...\n",
            "  Testing batch size 4...\n",
            "  Testing batch size 8...\n",
            "  Testing batch size 16...\n",
            "  Testing batch size 32...\n",
            "  Testing batch size 64...\n",
            "  Testing batch size 128...\n",
            "  Testing batch size 256...\n",
            "  Testing batch size 512...\n",
            "  Testing batch size 1028...\n",
            "  Testing batch size 2056...\n",
            "🧠 Running memory usage benchmark...\n",
            "\n",
            "📋 LARGE Results Summary:\n",
            "  Parameters: 3,504,323\n",
            "  Latency (single): 8.70ms ± 2.80ms\n",
            "  P95 latency: 12.92ms\n",
            "  Max throughput: 1897.0 samples/sec (batch=32)\n",
            "  Memory (batch=1): 2.1MB total, 2.1MB activations\n",
            "  Memory (batch=32): 105.5MB total, 3.30MB per sample\n",
            "\n",
            "💾 Results saved to: benchmark_results_20250930_021632.json\n",
            "\n",
            "🎯 BENCHMARK SUMMARY\n",
            "============================================================\n",
            " Model |   Params |  Latency | Throughput |   Memory\n",
            "------------------------------------------------------------\n",
            " SMALL |  462,147 |   5.49ms |    175.7 sps |    2.1MB\n",
            "MEDIUM | 1,506,051 |   6.52ms |    148.4 sps |    2.1MB\n",
            " LARGE | 3,504,323 |   8.70ms |    127.9 sps |    2.1MB\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Mamba KWS Inference Benchmark\n",
        "============================\n",
        "\n",
        "This script benchmarks inference performance for small, medium, and large Mamba models\n",
        "for Keyword Spotting using the exact implementations from the collab code files.\n",
        "It can run both locally (CPU) and on Colab (GPU).\n",
        "\n",
        "Features:\n",
        "- Tests latency over 1000 runs\n",
        "- Tests throughput for batch sizes: 1, 2, 4, 8, 16, 32\n",
        "- Uses exact Mamba implementation from collab code\n",
        "- Uses pre-trained model weights from models/ directory\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import time\n",
        "import statistics\n",
        "import json\n",
        "import math\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "# Detect environment\n",
        "IS_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in globals() else False\n",
        "HAS_CUDA = torch.cuda.is_available()\n",
        "\n",
        "print(f\"Environment: {'Colab' if IS_COLAB else 'Local'}\")\n",
        "print(f\"CUDA available: {HAS_CUDA}\")\n",
        "device = torch.device(\"cuda\" if HAS_CUDA else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Try to import mamba_ssm, with graceful fallback for local testing\n",
        "try:\n",
        "    from mamba_ssm import Mamba\n",
        "    print(\"✅ Using mamba_ssm library\")\n",
        "    USE_MAMBA_SSM = True\n",
        "except ImportError:\n",
        "    print(\"⚠️  mamba_ssm not available - will run benchmark with dummy Mamba for performance testing\")\n",
        "    USE_MAMBA_SSM = False\n",
        "\n",
        "    # Simple dummy Mamba for benchmarking computational load (not for accuracy)\n",
        "    class Mamba(nn.Module):\n",
        "        def __init__(self, d_model: int, d_state: int = 16, expand: int = 2):\n",
        "            super().__init__()\n",
        "            d_inner = d_model * expand\n",
        "            self.proj = nn.Linear(d_model, d_inner * 2)\n",
        "            self.conv1d = nn.Conv1d(d_inner, d_inner, 3, padding=1, groups=d_inner)\n",
        "            self.out_proj = nn.Linear(d_inner, d_model)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Simple approximation for benchmarking computational complexity\n",
        "            x_proj = self.proj(x)\n",
        "            x1, x2 = x_proj.chunk(2, dim=-1)\n",
        "            x1 = x1.transpose(1, 2)\n",
        "            x1 = self.conv1d(x1)\n",
        "            x1 = x1.transpose(1, 2)\n",
        "            x1 = F.silu(x1)\n",
        "            return self.out_proj(x1 * F.silu(x2))\n",
        "\n",
        "# =============================================================================\n",
        "# Audio Processing - Exact implementation from collab code\n",
        "# =============================================================================\n",
        "\n",
        "class WaveToSpec:\n",
        "    def __init__(self,\n",
        "                 feature_type: str = \"mel\",\n",
        "                 sample_rate: int = 16_000,\n",
        "                 n_fft: int = 2048,\n",
        "                 hop_length: int = 256,\n",
        "                 n_mels: int = 128,\n",
        "                 n_mfcc: int = 40,\n",
        "                 top_db: int | None = 80,\n",
        "                 apply_mask: bool = True,\n",
        "                 freq_mask_param: int = 15,\n",
        "                 time_mask_param: int = 10):\n",
        "        self.feature_type = feature_type.lower(); assert self.feature_type in {\"mel\",\"mfcc\"}\n",
        "        self.apply_mask = apply_mask and self.feature_type == \"mel\"\n",
        "\n",
        "        if self.feature_type == \"mel\":\n",
        "            self.spec = T.MelSpectrogram(sample_rate, n_fft, hop_length, n_mels, power=2)\n",
        "            self.to_db = T.AmplitudeToDB(stype=\"power\", top_db=top_db)\n",
        "            if self.apply_mask:\n",
        "                self.freq_mask = T.FrequencyMasking(freq_mask_param)\n",
        "                self.time_mask = T.TimeMasking(time_mask_param)\n",
        "        else:\n",
        "            self.spec = T.MFCC(sample_rate, n_mfcc,\n",
        "                                melkwargs=dict(n_fft=n_fft, hop_length=hop_length, n_mels=n_mels))\n",
        "            self.to_db = None\n",
        "            self.freq_mask = self.time_mask = None\n",
        "\n",
        "    def __call__(self, wav: torch.Tensor) -> torch.Tensor:\n",
        "        if wav.dim() == 1:\n",
        "            wav = wav.unsqueeze(0)\n",
        "        feats = self.spec(wav)\n",
        "        if self.to_db is not None:\n",
        "            feats = self.to_db(feats.clamp(min=1e-10))\n",
        "        if self.apply_mask:\n",
        "            # No masking during inference\n",
        "            pass\n",
        "        return feats\n",
        "\n",
        "# =============================================================================\n",
        "# Model Architecture - Exact implementation from collab code\n",
        "# =============================================================================\n",
        "\n",
        "class MambaKWS(nn.Module):\n",
        "    def __init__(self, num_classes: int, d_model=256, d_state=32, expand=2, n_layers=8, in_ch=1, feature_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional embedding layer for feature extraction\n",
        "        self.conv_embed = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.SiLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.SiLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.SiLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.SiLU(),\n",
        "            nn.MaxPool2d((2, 1)),\n",
        "        )\n",
        "\n",
        "        # Calculate the flattened dimension after convolutions to project to d_model\n",
        "        freq_dim_after_conv = feature_dim // 4\n",
        "        flattened_dim = 64 * freq_dim_after_conv\n",
        "\n",
        "        # Projection layer to map flattened conv features to Mamba's dimension\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(flattened_dim, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Add Mamba blocks with layer norm and residuals\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.ModuleDict({\n",
        "                \"norm\": nn.LayerNorm(d_model),\n",
        "                \"mamba\": Mamba(d_model=d_model, d_state=d_state, expand=expand),\n",
        "                \"dropout\": nn.Dropout(max(0.02, 0.05 - (i * 0.005)))\n",
        "            }) for i in range(n_layers)\n",
        "        ])\n",
        "        self.pre_classifier_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Classifier head\n",
        "        self.classifier_dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(d_model // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, lengths: torch.Tensor | None = None):  # x: [B, T, F]\n",
        "        # reshape for Conv2d: [B, T, F] -> [B, 1, F, T]\n",
        "        x = x.permute(0, 2, 1).unsqueeze(1)\n",
        "\n",
        "        # conv front-end\n",
        "        x = self.conv_embed(x)                  # [B, 64, F', T']\n",
        "\n",
        "        # flatten per time-step and project\n",
        "        x = x.permute(0, 3, 1, 2).contiguous().flatten(2)  # [B, T', 64*F']\n",
        "        x = self.proj(x)                                   # [B, T', d_model]\n",
        "\n",
        "        # Mamba blocks with residual connections\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            residual = x\n",
        "            x = blk[\"norm\"](x)\n",
        "            x = blk[\"mamba\"](x)\n",
        "            x = blk[\"dropout\"](x)\n",
        "            x = residual + x\n",
        "\n",
        "        x = self.pre_classifier_norm(x)\n",
        "\n",
        "        # mask-aware mean pooling over time\n",
        "        if lengths is not None:\n",
        "            t_lens = torch.div(lengths, 2, rounding_mode='floor').clamp(min=1).to(x.device)  # first pool halves time\n",
        "            Tprime = x.size(1)\n",
        "            mask = (torch.arange(Tprime, device=x.device)[None, :] < t_lens[:, None]).float()  # [B, T']\n",
        "            mask = mask.unsqueeze(-1)  # [B, T', 1]\n",
        "            x_sum = (x * mask).sum(dim=1)                            # [B, d_model]\n",
        "            denom = mask.sum(dim=1).clamp(min=1.0)                   # [B, 1]\n",
        "            pooled = x_sum / denom\n",
        "        else:\n",
        "            pooled = x.mean(dim=1)\n",
        "\n",
        "        # single-head output\n",
        "        main_output = self.classifier(self.classifier_dropout(pooled))\n",
        "        return main_output\n",
        "\n",
        "# =============================================================================\n",
        "# Model Configurations\n",
        "# =============================================================================\n",
        "\n",
        "MODEL_CONFIGS = {\n",
        "    'small': {\n",
        "        'model_path': 'content/small_kws_mel_97.42.pt',\n",
        "        'd_model': 64,\n",
        "        'd_state': 16,\n",
        "        'n_layers': 8,\n",
        "        'expected_classes': 35  # Google Speech Commands v0.02\n",
        "    },\n",
        "    'medium': {\n",
        "        'model_path': 'content/medium_kws_melSpec_97.58.pt',\n",
        "        'd_model': 128,\n",
        "        'd_state': 16,\n",
        "        'n_layers': 10,\n",
        "        'expected_classes': 35\n",
        "    },\n",
        "    'large': {\n",
        "        'model_path': 'content/large_kws_melSpec_97.75.pt',\n",
        "        'd_model': 192,\n",
        "        'd_state': 16,\n",
        "        'n_layers': 12,\n",
        "        'expected_classes': 35\n",
        "    }\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# Utility Functions\n",
        "# =============================================================================\n",
        "\n",
        "def load_model(model_size: str, device: torch.device) -> MambaKWS:\n",
        "    \"\"\"Load a pre-trained model with exact configuration from collab code\"\"\"\n",
        "    config = MODEL_CONFIGS[model_size]\n",
        "\n",
        "    # Create model with exact same parameters as collab code\n",
        "    model = MambaKWS(\n",
        "        num_classes=config['expected_classes'],\n",
        "        d_model=config['d_model'],\n",
        "        d_state=config['d_state'],\n",
        "        n_layers=config['n_layers'],\n",
        "        feature_dim=128  # Mel-spectrogram feature dimension\n",
        "    )\n",
        "\n",
        "    # Try to load weights if available\n",
        "    model_path = Path(config['model_path'])\n",
        "    if model_path.exists() and USE_MAMBA_SSM:\n",
        "        try:\n",
        "            checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "            # Handle different checkpoint formats\n",
        "            if isinstance(checkpoint, dict):\n",
        "                if 'model_state_dict' in checkpoint:\n",
        "                    state_dict = checkpoint['model_state_dict']\n",
        "                elif 'state_dict' in checkpoint:\n",
        "                    state_dict = checkpoint['state_dict']\n",
        "                else:\n",
        "                    state_dict = checkpoint\n",
        "            else:\n",
        "                state_dict = checkpoint\n",
        "\n",
        "            model.load_state_dict(state_dict, strict=True)\n",
        "            print(f\"✅ {model_size} model loaded successfully from {model_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Could not load {model_size} weights ({e}), using random weights for benchmarking\")\n",
        "    else:\n",
        "        if not model_path.exists():\n",
        "            print(f\"⚠️  Model file not found: {model_path}\")\n",
        "        if not USE_MAMBA_SSM:\n",
        "            print(f\"⚠️  Using dummy Mamba implementation\")\n",
        "        print(f\"Using random weights for {model_size} model (benchmark mode)\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_dummy_input(batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Create dummy mel-spectrogram input matching collab code preprocessing\"\"\"\n",
        "    # Typical mel-spectrogram dimensions from 1-second audio at 16kHz\n",
        "    # After conv layers: time dimension is roughly halved\n",
        "    T, F = 63, 128  # Time frames, Mel frequency bins\n",
        "    features = torch.randn(batch_size, T, F, device=device)\n",
        "    lengths = torch.full((batch_size,), T, dtype=torch.long, device=device)\n",
        "    return features, lengths\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    \"\"\"Count total trainable parameters\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def benchmark_memory_usage(model: nn.Module, device: torch.device,\n",
        "                          batch_sizes: List[int] = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1028, 2056]) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"Benchmark memory usage for CUDA inference\"\"\"\n",
        "    if device.type != 'cuda':\n",
        "        return {}\n",
        "\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    # Get baseline memory usage (model weights)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    baseline_memory = torch.cuda.memory_allocated()\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "        # Record memory before inference\n",
        "        memory_before = torch.cuda.memory_allocated()\n",
        "\n",
        "        # Run inference\n",
        "        dummy_input, dummy_lengths = create_dummy_input(batch_size, device)\n",
        "        with torch.no_grad():\n",
        "            _ = model(dummy_input, dummy_lengths)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Record peak memory during inference\n",
        "        peak_memory = torch.cuda.max_memory_allocated()\n",
        "        memory_after = torch.cuda.memory_allocated()\n",
        "\n",
        "        # Calculate memory usage\n",
        "        inference_memory = peak_memory - baseline_memory\n",
        "        activation_memory = peak_memory - memory_before\n",
        "\n",
        "        results[f\"batch_{batch_size}\"] = {\n",
        "            'baseline_mb': baseline_memory / (1024**2),\n",
        "            'peak_mb': peak_memory / (1024**2),\n",
        "            'inference_mb': inference_memory / (1024**2),\n",
        "            'activation_mb': activation_memory / (1024**2),\n",
        "            'memory_per_sample_mb': activation_memory / batch_size / (1024**2)\n",
        "        }\n",
        "\n",
        "        # Cleanup\n",
        "        del dummy_input, dummy_lengths\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "def benchmark_latency(model: nn.Module, device: torch.device, num_runs: int = 1000) -> Dict[str, float]:\n",
        "    \"\"\"Benchmark latency for single batch inference\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Warmup runs\n",
        "    dummy_input, dummy_lengths = create_dummy_input(1, device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            _ = model(dummy_input, dummy_lengths)\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Actual benchmark\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_runs):\n",
        "            dummy_input, dummy_lengths = create_dummy_input(1, device)\n",
        "\n",
        "            start_time = time.perf_counter()\n",
        "            _ = model(dummy_input, dummy_lengths)\n",
        "\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            end_time = time.perf_counter()\n",
        "            times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
        "\n",
        "    return {\n",
        "        'mean_ms': statistics.mean(times),\n",
        "        'median_ms': statistics.median(times),\n",
        "        'std_ms': statistics.stdev(times),\n",
        "        'min_ms': min(times),\n",
        "        'max_ms': max(times),\n",
        "        'p95_ms': np.percentile(times, 95),\n",
        "        'p99_ms': np.percentile(times, 99)\n",
        "    }\n",
        "\n",
        "def benchmark_throughput(model: nn.Module, device: torch.device,\n",
        "                        batch_sizes: List[int] = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1028, 2056]) -> Dict[int, Dict[str, float]]:\n",
        "    \"\"\"Benchmark throughput for different batch sizes\"\"\"\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        print(f\"  Testing batch size {batch_size}...\")\n",
        "\n",
        "        # Warmup\n",
        "        dummy_input, dummy_lengths = create_dummy_input(batch_size, device)\n",
        "        with torch.no_grad():\n",
        "            for _ in range(5):\n",
        "                _ = model(dummy_input, dummy_lengths)\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        # Benchmark\n",
        "        num_runs = max(10, 100 // batch_size)  # Fewer runs for larger batches\n",
        "        times = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_runs):\n",
        "                dummy_input, dummy_lengths = create_dummy_input(batch_size, device)\n",
        "\n",
        "                start_time = time.perf_counter()\n",
        "                _ = model(dummy_input, dummy_lengths)\n",
        "\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.synchronize()\n",
        "\n",
        "                end_time = time.perf_counter()\n",
        "                times.append(end_time - start_time)\n",
        "\n",
        "        avg_time = statistics.mean(times)\n",
        "        throughput = batch_size / avg_time\n",
        "\n",
        "        results[batch_size] = {\n",
        "            'avg_time_s': avg_time,\n",
        "            'throughput_samples_per_s': throughput,\n",
        "            'time_per_sample_ms': (avg_time / batch_size) * 1000\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_full_benchmark():\n",
        "    \"\"\"Run complete benchmark for all model sizes\"\"\"\n",
        "    print(f\"\\n🚀 Starting Mamba KWS Inference Benchmark\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Using mamba_ssm: {USE_MAMBA_SSM}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    results = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'device': str(device),\n",
        "        'cuda_available': HAS_CUDA,\n",
        "        'using_mamba_ssm': USE_MAMBA_SSM,\n",
        "        'models': {}\n",
        "    }\n",
        "\n",
        "    for model_size in ['small', 'medium', 'large']:\n",
        "        print(f\"\\n📊 Benchmarking {model_size.upper()} model...\")\n",
        "\n",
        "        # Load model\n",
        "        model = load_model(model_size, device)\n",
        "        param_count = count_parameters(model)\n",
        "\n",
        "        print(f\"Model parameters: {param_count:,}\")\n",
        "\n",
        "        # Latency benchmark\n",
        "        print(\"🔍 Running latency benchmark (1000 runs)...\")\n",
        "        latency_results = benchmark_latency(model, device)\n",
        "\n",
        "        # Throughput benchmark\n",
        "        print(\"📈 Running throughput benchmark...\")\n",
        "        throughput_results = benchmark_throughput(model, device)\n",
        "\n",
        "        # Memory benchmark (CUDA only)\n",
        "        memory_results = {}\n",
        "        if device.type == 'cuda':\n",
        "            print(\"🧠 Running memory usage benchmark...\")\n",
        "            memory_results = benchmark_memory_usage(model, device)\n",
        "\n",
        "        results['models'][model_size] = {\n",
        "            'parameters': param_count,\n",
        "            'config': MODEL_CONFIGS[model_size],\n",
        "            'latency': latency_results,\n",
        "            'throughput': throughput_results,\n",
        "            'memory': memory_results\n",
        "        }\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\n📋 {model_size.upper()} Results Summary:\")\n",
        "        print(f\"  Parameters: {param_count:,}\")\n",
        "        print(f\"  Latency (single): {latency_results['mean_ms']:.2f}ms ± {latency_results['std_ms']:.2f}ms\")\n",
        "        print(f\"  P95 latency: {latency_results['p95_ms']:.2f}ms\")\n",
        "        print(f\"  Max throughput: {throughput_results[32]['throughput_samples_per_s']:.1f} samples/sec (batch=32)\")\n",
        "\n",
        "        if memory_results:\n",
        "            batch_1_memory = memory_results['batch_1']\n",
        "            batch_32_memory = memory_results['batch_32']\n",
        "            print(f\"  Memory (batch=1): {batch_1_memory['inference_mb']:.1f}MB total, {batch_1_memory['activation_mb']:.1f}MB activations\")\n",
        "            print(f\"  Memory (batch=32): {batch_32_memory['inference_mb']:.1f}MB total, {batch_32_memory['memory_per_sample_mb']:.2f}MB per sample\")\n",
        "\n",
        "        # Cleanup\n",
        "        del model\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Save results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_file = f\"benchmark_results_{timestamp}.json\"\n",
        "\n",
        "    with open(results_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"\\n💾 Results saved to: {results_file}\")\n",
        "\n",
        "    # Print final summary\n",
        "    print(f\"\\n🎯 BENCHMARK SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    if device.type == 'cuda':\n",
        "        print(f\"{'Model':>6} | {'Params':>8} | {'Latency':>8} | {'Throughput':>10} | {'Memory':>8}\")\n",
        "        print(\"-\" * 60)\n",
        "        for model_size in ['small', 'medium', 'large']:\n",
        "            model_results = results['models'][model_size]\n",
        "            memory_str = f\"{model_results['memory']['batch_1']['inference_mb']:.1f}MB\" if model_results['memory'] else \"N/A\"\n",
        "            print(f\"{model_size.upper():>6} | {model_results['parameters']:>8,} | \"\n",
        "                  f\"{model_results['latency']['mean_ms']:>6.2f}ms | \"\n",
        "                  f\"{model_results['throughput'][1]['throughput_samples_per_s']:>8.1f} sps | \"\n",
        "                  f\"{memory_str:>8}\")\n",
        "    else:\n",
        "        for model_size in ['small', 'medium', 'large']:\n",
        "            model_results = results['models'][model_size]\n",
        "            print(f\"{model_size.upper():>6}: {model_results['parameters']:>8,} params | \"\n",
        "                  f\"{model_results['latency']['mean_ms']:>6.2f}ms | \"\n",
        "                  f\"{model_results['throughput'][1]['throughput_samples_per_s']:>6.1f} sps\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_full_benchmark()\n"
      ]
    }
  ]
}