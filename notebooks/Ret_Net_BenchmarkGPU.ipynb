{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U datasets\n",
        "!pip install datasets==2.16.0\n",
        "!pip install huggingface-hub==0.20.0\n",
        "!apt-get install -y libsox-dev\n",
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install yet-another-retnet==0.5.1\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4INgCOm4vzf",
        "outputId": "e62dbd9c-4a70-4e46-fcd0-320b4649ed4f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.16.0\n",
            "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.16.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.0)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (0.70.16)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.0)\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (3.12.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.16.0) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.0) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.16.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.16.0) (2025.8.3)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.16.0)\n",
            "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
            "  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.16.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.0) (1.17.0)\n",
            "Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pyarrow-hotfix, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.16\n",
            "    Uninstalling multiprocess-0.70.16:\n",
            "      Successfully uninstalled multiprocess-0.70.16\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.16.0 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15 pyarrow-hotfix-0.7\n",
            "Collecting huggingface-hub==0.20.0\n",
            "  Downloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (2023.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.20.0) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.20.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.20.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.20.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.20.0) (2025.8.3)\n",
            "Downloading huggingface_hub-0.20.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.34.4\n",
            "    Uninstalling huggingface-hub-0.34.4:\n",
            "      Successfully uninstalled huggingface-hub-0.34.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.17.1 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "transformers 4.56.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "accelerate 1.10.1 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "gradio 5.44.1 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "diffusers 0.35.1 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.20.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0\n",
            "  libsox-fmt-all libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base libsox-fmt-mp3\n",
            "  libsox-fmt-oss libsox-fmt-pulse libsox3 libwavpack1\n",
            "Suggested packages:\n",
            "  libaudio2 libsndio6.1\n",
            "The following NEW packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0\n",
            "  libsox-dev libsox-fmt-all libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base\n",
            "  libsox-fmt-mp3 libsox-fmt-oss libsox-fmt-pulse libsox3 libwavpack1\n",
            "0 upgraded, 16 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,053 kB of archives.\n",
            "After this operation, 4,061 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libao-common all 1.2.2+20180113-1.1ubuntu3 [6,568 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libao4 amd64 1.2.2+20180113-1.1ubuntu3 [35.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libid3tag0 amd64 0.15.1b-14 [31.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmad0 amd64 0.15.1b-10ubuntu1 [63.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [240 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [11.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-ao amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [7,740 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwavpack1 amd64 5.4.0-1build2 [83.7 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [33.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [17.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-oss amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [9,424 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-pulse amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [7,732 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-all amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [5,016 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-dev amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [356 kB]\n",
            "Fetched 1,053 kB in 2s (438 kB/s)\n",
            "Selecting previously unselected package libao-common.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libao-common_1.2.2+20180113-1.1ubuntu3_all.deb ...\n",
            "Unpacking libao-common (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libao4:amd64.\n",
            "Preparing to unpack .../01-libao4_1.2.2+20180113-1.1ubuntu3_amd64.deb ...\n",
            "Unpacking libao4:amd64 (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../02-libid3tag0_0.15.1b-14_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../03-libmad0_0.15.1b-10ubuntu1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../04-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../05-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../06-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../07-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-ao:amd64.\n",
            "Preparing to unpack .../08-libsox-fmt-ao_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-ao:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../09-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../10-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../11-libsox-fmt-mp3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-oss:amd64.\n",
            "Preparing to unpack .../12-libsox-fmt-oss_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-oss:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-pulse:amd64.\n",
            "Preparing to unpack .../13-libsox-fmt-pulse_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-pulse:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-all:amd64.\n",
            "Preparing to unpack .../14-libsox-fmt-all_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-all:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-dev:amd64.\n",
            "Preparing to unpack .../15-libsox-dev_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-dev:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-oss:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libao-common (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libao4:amd64 (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-ao:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-pulse:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-all:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-dev:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp312-cp312-linux_x86_64.whl (799.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m799.0/799.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.19.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp312-cp312-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2023.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (75.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.8.0+cu126\n",
            "    Uninstalling torchaudio-2.8.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.17.1 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.20.0 which is incompatible.\n",
            "accelerate 1.10.1 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0+cu121 torchaudio-2.4.0+cu121 torchvision-0.19.0+cu121 triton-3.0.0\n",
            "Collecting yet-another-retnet==0.5.1\n",
            "  Downloading yet_another_retnet-0.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from yet-another-retnet==0.5.1) (0.8.1)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from yet-another-retnet==0.5.1) (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (2023.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->yet-another-retnet==0.5.1) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->yet-another-retnet==0.5.1) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->yet-another-retnet==0.5.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.8->yet-another-retnet==0.5.1) (1.3.0)\n",
            "Downloading yet_another_retnet-0.5.1-py3-none-any.whl (406 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m406.7/406.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yet-another-retnet\n",
            "Successfully installed yet-another-retnet-0.5.1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVhXGw4X3lvg",
        "outputId": "26b6dad2-ba1e-42ee-a930-84417f666ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment: Colab\n",
            "CUDA available: True\n",
            "Device: cuda\n",
            "‚úÖ Using yet_another_retnet.MultiScaleRetention\n",
            "\n",
            "üöÄ Starting RetNet KWS Inference Benchmark (Parallel + Recurrent)\n",
            "Device: cuda | using_retnet_lib=True\n",
            "================================================================\n",
            "\n",
            "üìä Benchmarking MEDIUM model...\n",
            "‚úÖ medium model loaded from /content/drive/MyDrive/kws_models/best_kws_retnet-small.pt\n",
            "Model parameters: 1,626,692\n",
            "\n",
            "‚ñ∂Ô∏è  Mode: PARALLEL\n",
            "  üîç Latency (1000 runs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1236796466.py:330: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(mp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  üìà Throughput across batch sizes...\n",
            "  [parallel] Testing batch size 1...\n",
            "  [parallel] Testing batch size 2...\n",
            "  [parallel] Testing batch size 4...\n",
            "  [parallel] Testing batch size 8...\n",
            "  [parallel] Testing batch size 16...\n",
            "  [parallel] Testing batch size 32...\n",
            "  üß† Memory usage across batch sizes...\n",
            "\n",
            "‚ñ∂Ô∏è  Mode: RECURRENT\n",
            "  üîç Latency (1000 runs)...\n",
            "  üìà Throughput across batch sizes...\n",
            "  [recurrent] Testing batch size 1...\n",
            "  [recurrent] Testing batch size 2...\n",
            "  [recurrent] Testing batch size 4...\n",
            "  [recurrent] Testing batch size 8...\n",
            "  [recurrent] Testing batch size 16...\n",
            "  [recurrent] Testing batch size 32...\n",
            "  üß† Memory usage across batch sizes...\n",
            "\n",
            "üìã MEDIUM [PARALLEL] Summary:\n",
            "  Latency (single): 9.07ms ¬± 1.63ms | p95 12.98ms\n",
            "  Max throughput (batch=32): 3376.4 samples/sec\n",
            "  Memory (batch=1): 2.1MB total, 2.1MB activations\n",
            "  Memory (batch=32): 105.5MB total, 3.30MB per sample\n",
            "\n",
            "üìã MEDIUM [RECURRENT] Summary:\n",
            "  Latency (single): 206.03ms ¬± 32.27ms | p95 279.48ms\n",
            "  Max throughput (batch=32): 121.0 samples/sec\n",
            "  Memory (batch=1): 2.1MB total, 2.1MB activations\n",
            "  Memory (batch=32): 105.5MB total, 3.30MB per sample\n",
            "\n",
            "üíæ Results saved to: benchmark_results_retnet_bimode_20250916_160211.json\n",
            "\n",
            "üéØ BENCHMARK SUMMARY (PARALLEL vs RECURRENT)\n",
            "================================================================\n",
            " Model |   Params |      Mode |    Latency |  Throughput |     Memory\n",
            "---------------------------------------------------------------------\n",
            "MEDIUM | 1,626,692 |  PARALLEL |    9.07ms |      86.4 sps |      2.1MB\n",
            "MEDIUM | 1,626,692 | RECURRENT |  206.03ms |       4.7 sps |      2.1MB\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "RetNet KWS Inference Benchmark (Parallel + Recurrent)\n",
        "=====================================================\n",
        "\n",
        "What‚Äôs new vs previous version:\n",
        "- Runs *both* parallel and recurrent paths for each model size.\n",
        "- Recurrent is true step-by-step with per-layer state; if the library\n",
        "  lacks forward_recurrent we fall back to a light content-mixing step\n",
        "  so benchmarks still complete (timings are then approximate).\n",
        "- Identical outputs layout to your Mamba bench, mirrored for each mode.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import time, statistics, json, math\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "IS_COLAB = 'google.colab' in str(globals().get('get_ipython', lambda: None)())\n",
        "HAS_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if HAS_CUDA else \"cpu\")\n",
        "\n",
        "print(f\"Environment: {'Colab' if IS_COLAB else 'Local'}\")\n",
        "print(f\"CUDA available: {HAS_CUDA}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# (Optional) Ampere-friendly math\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Try RetNet lib; keep a graceful fallback\n",
        "try:\n",
        "    from yet_another_retnet.retention import MultiScaleRetention\n",
        "    print(\"‚úÖ Using yet_another_retnet.MultiScaleRetention\")\n",
        "    USE_RETNET_LIB = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  yet_another_retnet not available ({e}) - using dummy retention\")\n",
        "    USE_RETNET_LIB = False\n",
        "\n",
        "# =============================================================================\n",
        "# Frontend (not used for benchmarking speed; included for parity)\n",
        "# =============================================================================\n",
        "class WaveToSpec:\n",
        "    def __init__(self,\n",
        "                 feature_type: str = \"mel\",\n",
        "                 sample_rate: int = 16_000,\n",
        "                 n_fft: int = 2048,\n",
        "                 hop_length: int = 256,\n",
        "                 n_mels: int = 128,\n",
        "                 n_mfcc: int = 40,\n",
        "                 top_db: int | None = 80,\n",
        "                 apply_mask: bool = False,\n",
        "                 freq_mask_param: int = 12,\n",
        "                 time_mask_param: int = 20):\n",
        "        self.feature_type = feature_type.lower(); assert self.feature_type in {\"mel\",\"mfcc\"}\n",
        "        self.apply_mask = apply_mask and self.feature_type == \"mel\"\n",
        "        if self.feature_type == \"mel\":\n",
        "            self.spec = T.MelSpectrogram(sample_rate, n_fft, hop_length, n_mels, power=2)\n",
        "            self.to_db = T.AmplitudeToDB(stype=\"power\", top_db=top_db)\n",
        "            if self.apply_mask:\n",
        "                self.freq_mask = T.FrequencyMasking(freq_mask_param)\n",
        "                self.time_mask = T.TimeMasking(time_mask_param)\n",
        "        else:\n",
        "            self.spec = T.MFCC(sample_rate, n_mfcc,\n",
        "                               melkwargs=dict(n_fft=n_fft, hop_length=hop_length, n_mels=n_mels))\n",
        "            self.to_db = None\n",
        "            self.freq_mask = self.time_mask = None\n",
        "\n",
        "    def __call__(self, wav: torch.Tensor) -> torch.Tensor:\n",
        "        if wav.dim() == 1:\n",
        "            wav = wav.unsqueeze(0)\n",
        "        feats = self.spec(wav)\n",
        "        if self.apply_mask:\n",
        "            feats = self.freq_mask(feats); feats = self.time_mask(feats)\n",
        "        if self.to_db is not None:\n",
        "            feats = self.to_db(feats.clamp(min=1e-10))\n",
        "        return feats\n",
        "\n",
        "# =============================================================================\n",
        "# RetNet model with dual-mode forward (parallel / recurrent)\n",
        "# =============================================================================\n",
        "class ChannelGroupNorm(nn.Module):\n",
        "    \"\"\"GroupNorm over channels for [B, T, D] tensors (channels-last).\"\"\"\n",
        "    def __init__(self, num_groups: int, num_channels: int, eps: float = 1e-5, affine: bool = True):\n",
        "        super().__init__()\n",
        "        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels, eps=eps, affine=affine)\n",
        "    def forward(self, x):  # x: [B, T, D]\n",
        "        return self.gn(x.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "class DummyRetention(nn.Module):\n",
        "    \"\"\"Stand-in when RetNet lib is missing; supports both parallel + 'recurrent' step.\"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int):\n",
        "        super().__init__()\n",
        "        self.lin_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.proj  = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    # Parallel-ish content mixing\n",
        "    def forward_parallel(self, q, k, v):  # [B,T,D]\n",
        "        attn = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(q.size(-1))\n",
        "        y = torch.matmul(F.softmax(attn, dim=-1), v)\n",
        "        return self.proj(y), None\n",
        "\n",
        "    # Recurrent step with a simple running KV accumulator (approximation)\n",
        "    def forward_recurrent(self, q_t, k_t, v_t, idx: int, state: Optional[Dict[str, torch.Tensor]]):\n",
        "        # q_t,k_t,v_t: [B,D]\n",
        "        if state is None:\n",
        "            state = {'K': k_t, 'KV': torch.bmm(k_t.unsqueeze(2), v_t.unsqueeze(1))}  # [B,D,D]\n",
        "        else:\n",
        "            state['K'] = state['K'] + k_t\n",
        "            state['KV'] = state['KV'] + torch.bmm(k_t.unsqueeze(2), v_t.unsqueeze(1))\n",
        "        # Simple linearized attention-style readout\n",
        "        denom = (torch.sum(q_t * state['K'], dim=-1, keepdim=True) + 1e-6)\n",
        "        y_t = torch.bmm(state['KV'], q_t.unsqueeze(2)).squeeze(2) / denom\n",
        "        return self.proj(y_t), state\n",
        "\n",
        "class RetNetBlock(nn.Module):\n",
        "    \"\"\"Pre-norm RetNet block that supports both parallel and recurrent modes.\"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int, drop: float):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.norm = ChannelGroupNorm(num_groups=n_heads, num_channels=d_model)\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        if USE_RETNET_LIB:\n",
        "            self.retention = MultiScaleRetention(embed_dim=d_model, num_heads=n_heads, relative_position=False)\n",
        "        else:\n",
        "            self.retention = DummyRetention(d_model, n_heads)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(drop)\n",
        "\n",
        "        self.ffn_norm = ChannelGroupNorm(num_groups=n_heads, num_channels=d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, 2 * d_model),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(2 * d_model, d_model),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "    def forward_parallel(self, x):  # x: [B, T, D]\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        q = F.normalize(self.q_proj(x), dim=-1)\n",
        "        k = F.normalize(self.k_proj(x), dim=-1)\n",
        "        v = self.v_proj(x)\n",
        "        y, _ = self.retention.forward_parallel(q, k, v)\n",
        "        y = self.out_proj(y)\n",
        "        x = residual + self.dropout(y)\n",
        "\n",
        "        residual = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = residual + self.ffn(x)\n",
        "        return x\n",
        "\n",
        "    def step_recurrent(self, h_t, t_idx: int, state=None):\n",
        "        \"\"\"\n",
        "        One recurrent step.\n",
        "        h_t: [B, D]  (single timestep input)\n",
        "        state: per-layer retention state (library-specific object or dict)\n",
        "        Returns: h_out_t [B, D], new_state\n",
        "        \"\"\"\n",
        "        # Retention sublayer (pre-norm on a 'length-1' time dimension)\n",
        "        residual = h_t\n",
        "        x1 = self.norm(h_t.unsqueeze(1)).squeeze(1)   # [B,D]\n",
        "        q_t = F.normalize(self.q_proj(x1), dim=-1)\n",
        "        k_t = F.normalize(self.k_proj(x1), dim=-1)\n",
        "        v_t = self.v_proj(x1)\n",
        "\n",
        "        new_state = None\n",
        "        if USE_RETNET_LIB:\n",
        "            # Try native recurrent; fall back to dummy if unavailable\n",
        "            if hasattr(self.retention, \"forward_recurrent\"):\n",
        "                y_t, new_state = self.retention.forward_recurrent(q_t, k_t, v_t, t_idx, state)\n",
        "            else:\n",
        "                # Parallel one-step fallback (computationally heavier per step)\n",
        "                y1, _ = self.retention.forward_parallel(q_t.unsqueeze(1), k_t.unsqueeze(1), v_t.unsqueeze(1))\n",
        "                y_t = y1.squeeze(1)\n",
        "                new_state = state\n",
        "        else:\n",
        "            y_t, new_state = self.retention.forward_recurrent(q_t, k_t, v_t, t_idx, state)\n",
        "\n",
        "        y_t = self.out_proj(y_t)\n",
        "        h_t = residual + self.dropout(y_t)\n",
        "\n",
        "        # FFN\n",
        "        residual = h_t\n",
        "        x2 = self.ffn_norm(h_t.unsqueeze(1)).squeeze(1)\n",
        "        h_t = residual + self.ffn(x2)\n",
        "        return h_t, new_state\n",
        "\n",
        "class RetNetKWS(nn.Module):\n",
        "    def __init__(self, num_classes: int, d_model=256, n_layers=8, n_heads=8, in_ch=1, feature_dim=128):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Conv front-end\n",
        "        self.conv_embed = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.SiLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.SiLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.SiLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.SiLU(),\n",
        "            nn.MaxPool2d((2, 1)),\n",
        "        )\n",
        "        freq_dim_after_conv = feature_dim // 4\n",
        "        flattened_dim = 64 * freq_dim_after_conv\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(flattened_dim, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            RetNetBlock(d_model=d_model, n_heads=n_heads, drop=max(0.02, 0.03 - (i * 0.003)))\n",
        "            for i in range(n_layers)\n",
        "        ])\n",
        "        self.pre_classifier_norm = nn.LayerNorm(d_model)\n",
        "        self.classifier_dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(d_model // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def _embed(self, x):  # x: [B, T, F]\n",
        "        x = x.permute(0, 2, 1).unsqueeze(1)          # [B,1,F,T]\n",
        "        x = self.conv_embed(x)                        # [B,64,F',T']\n",
        "        x = x.permute(0, 3, 1, 2).contiguous().flatten(2)  # [B,T',64*F']\n",
        "        x = self.proj(x)                              # [B,T',D]\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, lengths: torch.Tensor | None = None, mode: str = \"parallel\"):\n",
        "        \"\"\"\n",
        "        mode: 'parallel' | 'recurrent'\n",
        "        \"\"\"\n",
        "        x = self._embed(x)  # [B, T', D]\n",
        "        B, Tprime, D = x.shape\n",
        "\n",
        "        if mode == \"parallel\":\n",
        "            for blk in self.blocks:\n",
        "                x = blk.forward_parallel(x)\n",
        "            x = self.pre_classifier_norm(x)\n",
        "\n",
        "            if lengths is not None:\n",
        "                # time halved once by first MaxPool2d(2)\n",
        "                t_lens = torch.div(lengths, 2, rounding_mode='floor').clamp(min=1).to(x.device)\n",
        "                mask = (torch.arange(Tprime, device=x.device)[None, :] < t_lens[:, None]).float().unsqueeze(-1)\n",
        "                x_sum = (x * mask).sum(dim=1)\n",
        "                denom = mask.sum(dim=1).clamp(min=1.0)\n",
        "                pooled = x_sum / denom\n",
        "            else:\n",
        "                pooled = x.mean(dim=1)\n",
        "\n",
        "            logits = self.classifier(self.classifier_dropout(pooled))\n",
        "            return logits\n",
        "\n",
        "        elif mode == \"recurrent\":\n",
        "            # Process step-by-step and pool on the fly with a mask\n",
        "            # Track per-layer states (library-specific objects or dicts)\n",
        "            states: List[Optional[object]] = [None] * self.n_layers\n",
        "            # Running sum for masked mean\n",
        "            if lengths is not None:\n",
        "                t_lens = torch.div(lengths, 2, rounding_mode='floor').clamp(min=1).to(x.device)\n",
        "            else:\n",
        "                t_lens = torch.full((B,), Tprime, dtype=torch.long, device=x.device)\n",
        "            run_sum = torch.zeros(B, D, device=x.device)\n",
        "            denom = torch.zeros(B, 1, device=x.device)\n",
        "\n",
        "            for t in range(Tprime):\n",
        "                h_t = x[:, t, :]  # [B,D]\n",
        "                for li, blk in enumerate(self.blocks):\n",
        "                    h_t, states[li] = blk.step_recurrent(h_t, t_idx=t, state=states[li])\n",
        "                h_t = self.pre_classifier_norm(h_t)\n",
        "                # masked accumulation\n",
        "                use_t = (t < t_lens).float().unsqueeze(1)   # [B,1]\n",
        "                run_sum = run_sum + h_t * use_t\n",
        "                denom = denom + use_t\n",
        "\n",
        "            pooled = run_sum / denom.clamp(min=1.0)\n",
        "            logits = self.classifier(self.classifier_dropout(pooled))\n",
        "            return logits\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "# =============================================================================\n",
        "# Model configs (update paths to your checkpoints if needed)\n",
        "# =============================================================================\n",
        "MODEL_CONFIGS = {\n",
        "    'medium':  {'model_path': '/content/drive/MyDrive/kws_models/best_kws_retnet-small.pt', 'd_model': 128,  'n_layers': 6,  'n_heads': 8, 'expected_classes': 36},\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# Utilities & Bench code (now mode-aware)\n",
        "# =============================================================================\n",
        "def load_model(model_size: str, device: torch.device) -> RetNetKWS:\n",
        "    cfg = MODEL_CONFIGS[model_size]\n",
        "    model = RetNetKWS(\n",
        "        num_classes=cfg['expected_classes'],\n",
        "        d_model=cfg['d_model'],\n",
        "        n_layers=cfg['n_layers'],\n",
        "        n_heads=cfg['n_heads'],\n",
        "        feature_dim=128\n",
        "    ).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    mp = Path(cfg['model_path'])\n",
        "    if mp.exists():\n",
        "        try:\n",
        "            ckpt = torch.load(mp, map_location=device)\n",
        "            if isinstance(ckpt, dict):\n",
        "                if 'model_state_dict' in ckpt:\n",
        "                    state_dict = ckpt['model_state_dict']\n",
        "                elif 'state_dict' in ckpt:\n",
        "                    state_dict = ckpt['state_dict']\n",
        "                else:\n",
        "                    state_dict = ckpt\n",
        "            else:\n",
        "                state_dict = ckpt\n",
        "            model.load_state_dict(state_dict, strict=False)\n",
        "            print(f\"‚úÖ {model_size} model loaded from {mp}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Could not load weights for {model_size} ({e}); using random weights\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Model file not found for {model_size}: {mp}. Using random weights.\")\n",
        "    return model\n",
        "\n",
        "def create_dummy_input(batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    # Matches your mel front-end: ~63 frames, 128 Mel bins\n",
        "    T, F = 63, 128\n",
        "    x = torch.randn(batch_size, T, F, device=device)\n",
        "    lengths = torch.full((batch_size,), T, dtype=torch.long, device=device)\n",
        "    return x, lengths\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def _forward_once(model, x, lengths, mode: str):\n",
        "    return model(x, lengths=lengths, mode=mode)\n",
        "\n",
        "def benchmark_memory_usage(model: nn.Module, device: torch.device,\n",
        "                           mode: str,\n",
        "                           batch_sizes: List[int] = [1, 2, 4, 8, 16, 32]) -> Dict[str, Dict[str, float]]:\n",
        "    if device.type != 'cuda':\n",
        "        return {}\n",
        "    model.eval()\n",
        "    results = {}\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    baseline_memory = torch.cuda.memory_allocated()\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        memory_before = torch.cuda.memory_allocated()\n",
        "\n",
        "        x, l = create_dummy_input(bs, device)\n",
        "        with torch.no_grad():\n",
        "            _ = _forward_once(model, x, l, mode=mode)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        peak = torch.cuda.max_memory_allocated()\n",
        "        inference_memory = peak - baseline_memory\n",
        "        activation_memory = peak - memory_before\n",
        "\n",
        "        results[f\"batch_{bs}\"] = {\n",
        "            'baseline_mb': baseline_memory / (1024**2),\n",
        "            'peak_mb': peak / (1024**2),\n",
        "            'inference_mb': inference_memory / (1024**2),\n",
        "            'activation_mb': activation_memory / (1024**2),\n",
        "            'memory_per_sample_mb': activation_memory / bs / (1024**2)\n",
        "        }\n",
        "        del x, l\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "def benchmark_latency(model: nn.Module, device: torch.device,\n",
        "                      mode: str, num_runs: int = 1000) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    # Warmup\n",
        "    x, l = create_dummy_input(1, device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            _ = _forward_once(model, x, l, mode=mode)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    times_ms = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_runs):\n",
        "            x, l = create_dummy_input(1, device)\n",
        "            start = time.perf_counter()\n",
        "            _ = _forward_once(model, x, l, mode=mode)\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "            end = time.perf_counter()\n",
        "            times_ms.append((end - start) * 1000.0)\n",
        "\n",
        "    return {\n",
        "        'mean_ms': statistics.mean(times_ms),\n",
        "        'median_ms': statistics.median(times_ms),\n",
        "        'std_ms': statistics.stdev(times_ms),\n",
        "        'min_ms': min(times_ms),\n",
        "        'max_ms': max(times_ms),\n",
        "        'p95_ms': float(np.percentile(times_ms, 95)),\n",
        "        'p99_ms': float(np.percentile(times_ms, 99)),\n",
        "    }\n",
        "\n",
        "def benchmark_throughput(model: nn.Module, device: torch.device,\n",
        "                         mode: str, batch_sizes: List[int] = [1, 2, 4, 8, 16, 32]) -> Dict[int, Dict[str, float]]:\n",
        "    model.eval()\n",
        "    results = {}\n",
        "    for bs in batch_sizes:\n",
        "        print(f\"  [{mode}] Testing batch size {bs}...\")\n",
        "        # Warmup\n",
        "        x, l = create_dummy_input(bs, device)\n",
        "        with torch.no_grad():\n",
        "            for _ in range(5):\n",
        "                _ = _forward_once(model, x, l, mode=mode)\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        num_runs = max(10, 100 // bs)\n",
        "        times = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_runs):\n",
        "                x, l = create_dummy_input(bs, device)\n",
        "                start = time.perf_counter()\n",
        "                _ = _forward_once(model, x, l, mode=mode)\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.synchronize()\n",
        "                end = time.perf_counter()\n",
        "                times.append(end - start)\n",
        "\n",
        "        avg_time = statistics.mean(times)\n",
        "        results[bs] = {\n",
        "            'avg_time_s': avg_time,\n",
        "            'throughput_samples_per_s': bs / avg_time,\n",
        "            'time_per_sample_ms': (avg_time / bs) * 1000.0\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# Runner\n",
        "# =============================================================================\n",
        "def run_full_benchmark():\n",
        "    print(f\"\\nüöÄ Starting RetNet KWS Inference Benchmark (Parallel + Recurrent)\")\n",
        "    print(f\"Device: {device} | using_retnet_lib={USE_RETNET_LIB}\")\n",
        "    print(\"=\" * 64)\n",
        "\n",
        "    results = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'device': str(device),\n",
        "        'cuda_available': HAS_CUDA,\n",
        "        'using_retnet_lib': USE_RETNET_LIB,\n",
        "        'models': {}\n",
        "    }\n",
        "\n",
        "    for model_size in ['medium']:\n",
        "        print(f\"\\nüìä Benchmarking {model_size.upper()} model...\")\n",
        "        model = load_model(model_size, device)\n",
        "        params = count_parameters(model)\n",
        "        cfg = MODEL_CONFIGS[model_size]\n",
        "        print(f\"Model parameters: {params:,}\")\n",
        "\n",
        "        mode_results = {}\n",
        "        for mode in ['parallel', 'recurrent']:\n",
        "            print(f\"\\n‚ñ∂Ô∏è  Mode: {mode.upper()}\")\n",
        "            print(\"  üîç Latency (1000 runs)...\")\n",
        "            lat = benchmark_latency(model, device, mode=mode)\n",
        "\n",
        "            print(\"  üìà Throughput across batch sizes...\")\n",
        "            thr = benchmark_throughput(model, device, mode=mode)\n",
        "\n",
        "            mem = {}\n",
        "            if device.type == 'cuda':\n",
        "                print(\"  üß† Memory usage across batch sizes...\")\n",
        "                mem = benchmark_memory_usage(model, device, mode=mode)\n",
        "\n",
        "            mode_results[mode] = {\n",
        "                'latency': lat,\n",
        "                'throughput': thr,\n",
        "                'memory': mem\n",
        "            }\n",
        "\n",
        "        results['models'][model_size] = {\n",
        "            'parameters': params,\n",
        "            'config': cfg,\n",
        "            'parallel': mode_results['parallel'],\n",
        "            'recurrent': mode_results['recurrent'],\n",
        "        }\n",
        "\n",
        "        # Pretty per-model summary\n",
        "        for mode in ['parallel', 'recurrent']:\n",
        "            m = mode_results[mode]\n",
        "            t32 = m['throughput'][32]['throughput_samples_per_s']\n",
        "            print(f\"\\nüìã {model_size.upper()} [{mode.upper()}] Summary:\")\n",
        "            print(f\"  Latency (single): {m['latency']['mean_ms']:.2f}ms ¬± {m['latency']['std_ms']:.2f}ms | p95 {m['latency']['p95_ms']:.2f}ms\")\n",
        "            print(f\"  Max throughput (batch=32): {t32:.1f} samples/sec\")\n",
        "            if m['memory']:\n",
        "                b1 = m['memory']['batch_1']\n",
        "                b32 = m['memory']['batch_32']\n",
        "                print(f\"  Memory (batch=1): {b1['inference_mb']:.1f}MB total, {b1['activation_mb']:.1f}MB activations\")\n",
        "                print(f\"  Memory (batch=32): {b32['inference_mb']:.1f}MB total, {b32['memory_per_sample_mb']:.2f}MB per sample\")\n",
        "\n",
        "        del model\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    out_file = f\"benchmark_results_retnet_bimode_{ts}.json\"\n",
        "    with open(out_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"\\nüíæ Results saved to: {out_file}\")\n",
        "\n",
        "    # Final table\n",
        "    print(f\"\\nüéØ BENCHMARK SUMMARY (PARALLEL vs RECURRENT)\")\n",
        "    print(\"=\" * 64)\n",
        "    header = f\"{'Model':>6} | {'Params':>8} | {'Mode':>9} | {'Latency':>10} | {'Throughput':>11} | {'Memory':>10}\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for model_size in ['medium']:\n",
        "        mr = results['models'][model_size]\n",
        "        for mode in ['parallel', 'recurrent']:\n",
        "            m = mr[mode]\n",
        "            mem = \"N/A\"\n",
        "            if m['memory']:\n",
        "                mem = f\"{m['memory']['batch_1']['inference_mb']:.1f}MB\"\n",
        "            print(f\"{model_size.upper():>6} | {mr['parameters']:>8,} | {mode[:9].upper():>9} | \"\n",
        "                  f\"{m['latency']['mean_ms']:>7.2f}ms | {m['throughput'][1]['throughput_samples_per_s']:>9.1f} sps | {mem:>10}\")\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_full_benchmark()"
      ]
    }
  ]
}